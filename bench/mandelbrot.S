#if 0

void
mandelbrot_rvv(size_t width, size_t maxIter, uint32_t *res)
{
	vfloat32m2_t cx, cy, zx, zy, zx2, zy2;
	vuint32m2_t viter;
	vbool16_t mask;

	for (size_t y = 0; y < width; ++y) {
		size_t vl, x = width;
		while (x > 0) {
			x -= vl = __riscv_vsetvl_e32m2(x);

			mask = __riscv_vmset_m_b16(vl);
			viter = __riscv_vmv_v_x_u32m2(0, vl);

			cx = __riscv_vfcvt_f_xu_v_f32m2(__riscv_vadd_vx_u32m2(__riscv_viota_m_u32m2(mask, vl), x, vl), vl);
			cy = __riscv_vfmv_v_f_f32m2(y, vl);

			cx = __riscv_vfadd_vf_f32m2(__riscv_vfmul_vf_f32m2(cx, 2.0f / width, vl), -1.5f, vl);
			cy = __riscv_vfadd_vf_f32m2(__riscv_vfmul_vf_f32m2(cy, 2.0f / width, vl), -1, vl);

			zx = zy = zx2 = zy2 = __riscv_vfmv_v_f_f32m2(0, vl);

			size_t iter = 0;
			while (iter < maxIter && __riscv_vfirst_m_b16(mask, vl) >= 0) {
				mask = __riscv_vmflt_vf_f32m2_b16(__riscv_vfadd_vv_f32m2(zx2, zy2, vl), 4, vl);
				zx2 = __riscv_vfadd_vv_f32m2(__riscv_vfsub_vv_f32m2(zx2, zy2, vl), cx, vl);
				zy = __riscv_vfmacc_vv_f32m2(cy, __riscv_vfadd_vv_f32m2(zx, zx, vl), zy, vl);
				zx = zx2;
				zx2 = __riscv_vfmul_vv_f32m2(zx, zx, vl);
				zy2 = __riscv_vfmul_vv_f32m2(zy, zy, vl);
				viter = __riscv_vadd_vx_u32m2_m(mask, viter, 1, vl);
				++iter;
			}
			__riscv_vse32_v_u32m2(res + x, viter, vl);
		}
		res += width;
	}
}
#endif

#if MX_N > 0 && MX_N <= 2

.global MX(mandelbrot_rvv_f32_) # generated by clang
MX(mandelbrot_rvv_f32_):
	beqz a0, MX(rvv_f32_13)
	beqz a1, MX(rvv_f32_9)
	li a7, 0
	fcvt.s.lu fa5, a0
	lui a3, 262144
	fmv.w.x fa4, a3
	fdiv.s fa5, fa4, fa5
	lui a3, 785408
	fmv.w.x fa4, a3
	lui a3, 784384
	fmv.w.x fa3, a3
	lui a3, 264192
	fmv.w.x fa2, a3
	slli a6, a0, 2
	j MX(rvv_f32_4)
MX(rvv_f32_3):
	addi a7, a7, 1
	add a2, a2, a6
	beq a7, a0, MX(rvv_f32_13)
MX(rvv_f32_4):
	fcvt.s.lu fa1, a7
	mv t0, a0
	j MX(rvv_f32_6)
MX(rvv_f32_5):
	slli a3, t0, 2
	add a3, a3, a2
	vsetvli zero, zero, e32, MX(), ta, ma
	vse32.v v8, (a3)
	beqz t0, MX(rvv_f32_3)
MX(rvv_f32_6):
	vsetvli a4, t0, e32, MX(), ta, ma
	sub t0, t0, a4
	vmset.m v0
	vmv.v.i v8, 0
	viota.m v10, v0
	vadd.vx v10, v10, t0
	vfcvt.f.xu.v v10, v10
	vfmv.v.f v12, fa1
	vfmul.vf v10, v10, fa5
	vfadd.vf v10, v10, fa4
	vfmul.vf v12, v12, fa5
	vmv.v.i v18, 0
	vfadd.vf v12, v12, fa3
	mv a3, a1
	vmv.v.i v14, 0
	vmv.v.i v16, 0
	vmv.v.i v20, 0
MX(rvv_f32_7):
#if HAS_RVV_1_0
	vsetvli zero, a4, e8, mf2, ta, ma
#else
	vsetvli zero, a4, e8, m1, ta, ma
#endif
	vfirst.m a5, v0
	bltz a5, MX(rvv_f32_5)
	vsetvli zero, zero, e32, MX(), ta, ma
	vfadd.vv v22, v16, v20
	vmflt.vf v0, v22, fa2
	vfsub.vv v16, v16, v20
	vfadd.vv v18, v18, v18
	vfadd.vv v22, v16, v10
	vfmadd.vv v14, v18, v12
	vfmul.vv v16, v22, v22
	vfmul.vv v20, v14, v14
	addi a3, a3, -1
	vadd.vi v8, v8, 1, v0.t
	vmv.v.v v18, v22
	bnez a3, MX(rvv_f32_7)
	j MX(rvv_f32_5)
MX(rvv_f32_9):
	slli a3, a0, 2
MX(rvv_f32_10):
	mv a4, a0
MX(rvv_f32_11):
	vsetvli a5, a4, e32, MX(), ta, ma
	sub a4, a4, a5
	vmv.v.i v8, 0
	slli a5, a4, 2
	add a5, a5, a2
	vse32.v v8, (a5)
	bnez a4, MX(rvv_f32_11)
	addi a1, a1, 1
	add a2, a2, a3
	bne a1, a0, MX(rvv_f32_10)
MX(rvv_f32_13):
	ret


#endif
